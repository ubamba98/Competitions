{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "import timm\n",
    "import cv2\n",
    "from functools import partial\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_format(path, convert_mode, after_open)->Image:\n",
    "    image = np.load(path)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255)) #return fastai Image format\n",
    "\n",
    "vision.data.open_image = _load_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df['id_code'] = df['id_code'].map(lambda x: os.path.join(\"train_rc\",'{}.npy'.format(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"sample_submission.csv\")\n",
    "submit['id_code'] = submit['id_code'].map(lambda x: os.path.join(\"train_rc\",'{}.npy'.format(x)))\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, coef=[0.5, 1.5, 2.5, 3.5]):\n",
    "    X_p = np.copy(X)\n",
    "    for i, pred in enumerate(X_p):\n",
    "        if pred < coef[0]:\n",
    "            X_p[i] = 0\n",
    "        elif pred >= coef[0] and pred < coef[1]:\n",
    "            X_p[i] = 1\n",
    "        elif pred >= coef[1] and pred < coef[2]:\n",
    "            X_p[i] = 2\n",
    "        elif pred >= coef[2] and pred < coef[3]:\n",
    "            X_p[i] = 3\n",
    "        else:\n",
    "            X_p[i] = 4\n",
    "    return np.int8(X_p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranger Optimizer Taken from: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import itertools as it\n",
    "\n",
    "class Ranger(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95,0.999), eps=1e-8, weight_decay=0):\n",
    "        #parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    "        \n",
    "        #parameter comments:\n",
    "        # beta1 (momentum) of .95 seems to work better than .90...\n",
    "        #N_sma_threshold of 5 seems better in testing than 4.\n",
    "        #In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
    "        \n",
    "        #prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params,defaults)\n",
    "        \n",
    "        #adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    "        \n",
    "        #now we can get to work...\n",
    "        for group in self.param_groups:\n",
    "            group[\"step_counter\"] = 0\n",
    "            #print(\"group step counter init\")\n",
    "                      \n",
    "        #look ahead params\n",
    "        self.alpha = alpha\n",
    "        self.k = k \n",
    "        \n",
    "        #radam buffer for state\n",
    "        self.radam_buffer = [[None,None,None] for ind in range(10)]\n",
    "        \n",
    "        #lookahead weights\n",
    "        self.slow_weights = [[p.clone().detach() for p in group['params']]\n",
    "                                for group in self.param_groups]\n",
    "        \n",
    "        #don't use grad for lookahead weights\n",
    "        for w in it.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        \n",
    "    def __setstate__(self, state):\n",
    "        print(\"set state called\")\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "       \n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n",
    "        #Uncomment if you need to use the actual closure...\n",
    "        \n",
    "        #if closure is not None:\n",
    "            #loss = closure()\n",
    "            \n",
    "        #------------ radam\n",
    "        for group in self.param_groups:\n",
    "    \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "    \n",
    "                p_data_fp32 = p.data.float()\n",
    "    \n",
    "                state = self.state[p]\n",
    "    \n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "    \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "    \n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "    \n",
    "                state['step'] += 1\n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "    \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "    \n",
    "                if N_sma > 4:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "    \n",
    "                p.data.copy_(p_data_fp32)\n",
    "        \n",
    "        \n",
    "        #---------------- end radam step\n",
    "        \n",
    "        #look ahead tracking and updating if latest batch = k\n",
    "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
    "            group['step_counter'] += 1\n",
    "            if group['step_counter'] % self.k != 0:\n",
    "                continue\n",
    "            for p,q in zip(group['params'],slow_weights):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                q.data.add_(self.alpha,p.data - q.data)\n",
    "                p.data.copy_(q.data)\n",
    "            \n",
    "        \n",
    "            \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "def qk(y_pred, y):\n",
    "    return torch.tensor(cohen_kappa_score(predict(y_pred), y, weights='quadratic'), device='cuda:0')\n",
    "def accuracy(y_pred, y):\n",
    "    return torch.tensor(accuracy_score(predict(y_pred), y), device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_squared_error(pred:Tensor, targ:Tensor)->Rank0Tensor:\n",
    "    \"Mean squared error between `pred` and `targ`.\"\n",
    "    pred,targ = flatten_check(pred,targ)\n",
    "    mse=(pred-targ)** 2\n",
    "    weights=torch.cuda.FloatTensor([ 2.028809,  9.897297,  3.665666, 18.974093, 12.413559])\n",
    "    wmse=weights[targ[...,None].to(dtype=torch.int64)]*mse\n",
    "    return wmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, val_index in skf.split(df.id_code, df['diagnosis']):\n",
    "    opt=partial(Ranger)\n",
    "    print(28*\"====\")\n",
    "    sz=300\n",
    "    bs=32\n",
    "    tfms = get_transforms(do_flip=True,flip_vert=True, max_rotate=360, max_zoom=1.5, max_lighting=0.2)\n",
    "    data_fold = (ImageList.from_df(df=df,path='',cols='id_code') \n",
    "        .split_by_idxs(train_index, val_index)\n",
    "        .label_from_df(cols='diagnosis', label_cls=FloatList) \n",
    "        .transform(tfms,size=sz, padding_mode='border', resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=bs,num_workers=4) \n",
    "        .normalize(imagenet_stats)  \n",
    "        )\n",
    "    learn = cnn_learner(data_fold, timm.models.tf_efficientnet_b3, metrics=[qk, accuracy],callback_fns=ShowGraph).to_fp16()\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(10, max_lr=1e-6, callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='valid_loss', name='b3_stage-1-'+str(i))])\n",
    "    learn.load('b3_stage-1-'+str(i))\n",
    "    learn.opt_func=opt\n",
    "    learn.fit(20, callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='valid_loss', name='b3_stage-2-1-'+str(i)),\n",
    "                                      callbacks.CSVLogger(learn, filename = 'b3_stage-2-1-'+str(i)),\n",
    "                                     ])\n",
    "    learn.load('b3_stage-2-1-'+str(i))\n",
    "    learn.freeze_to(-5)\n",
    "    learn.fit(10, callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='valid_loss', name='b3_stage-2-2-'+str(i)),\n",
    "                                      callbacks.CSVLogger(learn, filename = 'b3_stage-2-2-'+str(i))\n",
    "                                     ])\n",
    "    learn.load('b3_stage-2-2-'+str(i))\n",
    "    learn.freeze_to(-1)\n",
    "\n",
    "    learn.fit(5, callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='valid_loss', name='b3_stage-2-3-'+str(i)),\n",
    "                                      callbacks.CSVLogger(learn, filename = 'b3_stage-2-3-'+str(i))\n",
    "                                     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
